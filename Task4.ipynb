{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVy8WrtiQ3/WOQzQ+Nz8MR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edsml-dg1018/CM-assessment-202324/blob/main/Task4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Data Assimilation with Nonlinear Compression\n",
        "\n",
        "## Objective\n",
        "The objective of this task is to use a data assimilation model in a reduced space obtained by a nonlinear compression algorithm (Autoencoder). We will integrate the satellite observation data into the model data to perform real-time adjustments and evaluate the performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "_UlxT3jVnSmD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ1l7xT4nFus"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Loading and Preprocessing\n"
      ],
      "metadata": {
        "id": "D07k2OaUpI9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "\n",
        "# Define file paths\n",
        "background_path = 'drive/MyDrive/BDA_Assessment/Ferguson_fire_background.npy'\n",
        "obs_path = 'drive/MyDrive/BDA_Assessment/Ferguson_fire_obs.npy'\n",
        "\n",
        "# Load the datasets\n",
        "background_data = np.load(background_path)\n",
        "obs_data = np.load(obs_path)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "background_data = scaler.fit_transform(background_data.reshape(-1, background_data.shape[-1])).reshape(background_data.shape)\n",
        "obs_data = scaler.transform(obs_data.reshape(-1, obs_data.shape[-1])).reshape(obs_data.shape)\n",
        "\n",
        "# Load the saved Autoencoder model from Task 2\n",
        "autoencoder = load_model('drive/MyDrive/BDA_Assessment/autoencoder.keras')\n",
        "\n",
        "# Function to extract the encoder part of the autoencoder\n",
        "def get_encoder(autoencoder):\n",
        "    encoder = tf.keras.models.Model(autoencoder.input, autoencoder.get_layer('dense').output)\n",
        "    return encoder\n",
        "\n",
        "encoder = get_encoder(autoencoder)\n"
      ],
      "metadata": {
        "id": "BJKRYACvpGEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Assimilation using Kalman Filter\n",
        "We will use the Kalman Filter for data assimilation. The choice of covariance matrices R and B will be motivated, and the Kalman Filter will be applied in the reduced space obtained by the Autoencoder.\n"
      ],
      "metadata": {
        "id": "AFBktVJlpNIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode background and observation data using the encoder\n",
        "background_data_encoded = encoder.predict(background_data)\n",
        "obs_data_encoded = encoder.predict(obs_data)\n",
        "\n",
        "# Define the covariance matrices\n",
        "observation_variance = np.var(obs_data_encoded)\n",
        "R = np.eye(obs_data_encoded.shape[1]) * observation_variance  # Observation noise covariance matrix\n",
        "\n",
        "background_variance = np.var(background_data_encoded)\n",
        "B = np.eye(background_data_encoded.shape[1]) * background_variance  # Background error covariance matrix\n"
      ],
      "metadata": {
        "id": "7oUEtJXopRWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to apply the Kalman Filter in the latent space\n",
        "def kalman_filter(background, observation, R, B):\n",
        "    # Prediction step\n",
        "    x_pred = background\n",
        "    P_pred = B\n",
        "\n",
        "    # Update step\n",
        "    K = P_pred @ np.linalg.inv(P_pred + R)\n",
        "    x_updated = x_pred + K @ (observation - background)\n",
        "    P_updated = (np.eye(K.shape[0]) - K) @ P_pred\n",
        "\n",
        "    return x_updated\n",
        "\n",
        "# Apply the Kalman Filter in the latent space\n",
        "start_time = time.time()\n",
        "assimilated_data_encoded = np.array([kalman_filter(b, o, R, B) for b, o in zip(background_data_encoded, obs_data_encoded)])\n",
        "end_time = time.time()\n",
        "assimilation_time = end_time - start_time\n",
        "\n",
        "# Decode the assimilated data\n",
        "assimilated_data = autoencoder.predict(assimilated_data_encoded.reshape(-1, 128))  # Assuming the latent space size is 128\n",
        "assimilated_data = scaler.inverse_transform(assimilated_data).reshape(background_data.shape)\n"
      ],
      "metadata": {
        "id": "MQy-h_swpXPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation of Assimilation\n",
        "# Calculate MSE in the latent space\n",
        "mse_latent = mean_squared_error(background_data_encoded.flatten(), assimilated_data_encoded.flatten())\n",
        "print(f\"MSE in the latent space: {mse_latent}\")\n",
        "\n",
        "# Calculate MSE in the physical space\n",
        "mse_physical = mean_squared_error(background_data.flatten(), assimilated_data.flatten())\n",
        "print(f\"MSE in the physical space after decompression: {mse_physical}\")\n",
        "\n",
        "# Calculate the execution time\n",
        "print(f\"Assimilation Execution Time: {assimilation_time} seconds\")\n"
      ],
      "metadata": {
        "id": "KcTFv4QBpY-5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YSjmrjqHpdTi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}